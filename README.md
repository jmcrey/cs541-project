# PET: Achieving Big Results with Small Language Models

This repository will host all the code for the CS541 Project, where I will mimic the results of the paper [Itâ€™s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://aclanthology.org/2021.naacl-main.185/) by using the paper's [official repository](https://github.com/timoschick/pet). The goal of the paper is to show that smaller, more practical language models can achieve state-of-the-art (SOTA) results on text-classification tasks using few-shot learning. In this case, they introduce and expand their novel framework "Pattern-Exploiting Training" (PET) to achieve SOTA results on various text classification tasks using the smaller `albert-xxlarge-v2` model.
